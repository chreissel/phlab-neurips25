trainer:
  accelerator: auto
  default_root_dir: runs/jetClass_parT_simCLRWithClassifier_T0.1/
  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        monitor: val/loss
        mode: min
        save_last: true
    - class_path: lightning.pytorch.callbacks.EarlyStopping
      init_args:
        monitor: val/loss
        patience: 20
        mode: min
  logger:
    class_path: lightning.pytorch.loggers.WandbLogger
  log_every_n_steps: 1
  max_epochs: 100
  limit_train_batches: 100
  limit_val_batches: 20

data:
  class_path: data.datasets.JetClassDataset
  init_args:
    classes: [qcd,ttbar,wqq,zqq]
    input_config: configs/jetclass_data_configs/JetClass_full.yaml
    batch_size: 1024
    num_workers: 1

optimizer:
  class_path: torch.optim.AdamW
  init_args:
    lr: 5e-4

lr_scheduler:
  class_path: torch.optim.lr_scheduler.CosineAnnealingLR
  init_args:
    T_max: 100
    eta_min: 1e-6

model:
  class_path: models.litmodels.JetClassSimCLRModel
  init_args:
    encoder:
      class_path: models.networks.ParticleTransformerModel
      init_args:
        input_dim: 17 # number of features per PF particle, set by the jetclass data config you use
        num_classes: 4 # dimension of contrastive space
        embed_dims: [128,512,128]
        pair_embed_dims: [64,64,64]
        num_heads: 8
        num_layers: 8
        num_cls_layers: 2
        fc_params: [[128,0.0]]
    projector:
      class_path: models.networks.MLP
      init_args:
        input_dim: 4
        hidden_dims: [4]
        output_dim: 4
    #classifier:
    #  class_path: models.networks.MLP
    #  init_args:
    #    input_dim: 4
    #    hidden_dims: [16,16]
    #    output_dim: 4 # number of classes - for JetClass we use 4 for [qcd, top, w, z]
    classifier: null
    lambda_classifier: 1.0
    temperature: 0.1
    sup_simclr: true